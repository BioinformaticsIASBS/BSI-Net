{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d72893e-e336-4f7c-9e34-5aa78a1c3fe1",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa199c9a-17ba-440b-9046-1d07ffb0b994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1/10 (10.0%)\n",
      "Processing Fold 2/10 (20.0%)\n",
      "Processing Fold 3/10 (30.0%)\n",
      "Processing Fold 4/10 (40.0%)\n",
      "Processing Fold 5/10 (50.0%)\n",
      "Processing Fold 6/10 (60.0%)\n",
      "Processing Fold 7/10 (70.0%)\n",
      "Processing Fold 8/10 (80.0%)\n",
      "Processing Fold 9/10 (90.0%)\n",
      "Processing Fold 10/10 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score, average_precision_score\n",
    "\n",
    "# Parameters for Random Forest\n",
    "nums_estimators = [50]\n",
    "criteria = ['gini']\n",
    "methods_max_features = ['sqrt']\n",
    "\n",
    "# Read inputs for labels, indices, and total matrices\n",
    "data_dir = '../Data/'\n",
    "labels_file_path = data_dir + '31_Y_ratio1.txt'\n",
    "combinations_file_path = data_dir + '31_XIndex_ratio1.txt'\n",
    "small_molecule_matrix_file = data_dir + 'total_small_drugs.txt'\n",
    "biotech_matrix_file = data_dir + 'total_biotech_drugs.txt'\n",
    "\n",
    "# Load labels\n",
    "def read_label_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        return np.array([int(line.strip()) for line in lines])\n",
    "\n",
    "# Load combinations\n",
    "def read_combinations_file(file_path):\n",
    "    combinations = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            biotech_idx, small_idx = map(int, line.strip().split())\n",
    "            combinations.append((biotech_idx, small_idx))\n",
    "    return combinations\n",
    "\n",
    "labels = read_label_file(labels_file_path)\n",
    "combinations = read_combinations_file(combinations_file_path)\n",
    "\n",
    "# Load feature matrices\n",
    "small_molecule_total_matrix = np.loadtxt(small_molecule_matrix_file, dtype=float)\n",
    "biotech_total_matrix = np.loadtxt(biotech_matrix_file, dtype=float)\n",
    "\n",
    "# Generate features based on combinations\n",
    "features = np.array([\n",
    "    np.concatenate((small_molecule_total_matrix[small_idx], biotech_total_matrix[biotech_idx]))\n",
    "    for biotech_idx, small_idx in combinations\n",
    "])\n",
    "\n",
    "# Validate dimensions\n",
    "assert features.shape[0] == len(labels), \"Mismatch between number of features and labels\"\n",
    "\n",
    "results_directory = '../RF_results'\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "RF_results = results_directory + '/RF'\n",
    "os.makedirs(RF_results, exist_ok=True)\n",
    "\n",
    "# Prepare multi-class task\n",
    "targets = labels\n",
    "num_classes = 32  # Explicitly setting the number of classes\n",
    "\n",
    "num_folds = 10\n",
    "kfolder = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "folds = list(kfolder.split(features, targets))\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(y_true, y_pred, y_probs, num_classes):\n",
    "    metrics = {\"Micro\": {}, \"Macro\": {}, \"Weighted\": {}}\n",
    "    \n",
    "    # Micro metrics\n",
    "    metrics[\"Micro\"][\"Accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    metrics[\"Micro\"][\"Precision\"], metrics[\"Micro\"][\"Recall\"], metrics[\"Micro\"][\"F1\"], _ = \\\n",
    "        precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    metrics[\"Micro\"][\"MCC\"] = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    # Handle potential ValueError in AUC computation\n",
    "    try:\n",
    "        metrics[\"Micro\"][\"AUC\"] = roc_auc_score(y_true, y_probs, average=\"micro\", multi_class=\"ovr\")\n",
    "        metrics[\"Macro\"][\"AUC\"] = roc_auc_score(y_true, y_probs, average=\"macro\", multi_class=\"ovr\")\n",
    "        metrics[\"Weighted\"][\"AUC\"] = roc_auc_score(y_true, y_probs, average=\"weighted\", multi_class=\"ovr\")\n",
    "    except ValueError:\n",
    "        metrics[\"Micro\"][\"AUC\"] = np.nan\n",
    "        metrics[\"Macro\"][\"AUC\"] = np.nan\n",
    "        metrics[\"Weighted\"][\"AUC\"] = np.nan\n",
    "\n",
    "    metrics[\"Micro\"][\"AUPR\"] = average_precision_score(y_true, y_probs, average=\"micro\")\n",
    "\n",
    "    # Macro metrics\n",
    "    precision_per_class, recall_per_class, f1_per_class, _ = \\\n",
    "        precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)\n",
    "    aupr_per_class = [\n",
    "        average_precision_score((np.array(y_true) == i).astype(int), np.array(y_probs)[:, i])\n",
    "        for i in range(num_classes)\n",
    "    ]\n",
    "\n",
    "    metrics[\"Macro\"][\"Precision\"] = np.mean(precision_per_class)\n",
    "    metrics[\"Macro\"][\"Recall\"] = np.mean(recall_per_class)\n",
    "    metrics[\"Macro\"][\"F1\"] = np.mean(f1_per_class)\n",
    "    metrics[\"Macro\"][\"MCC\"] = matthews_corrcoef(y_true, y_pred)\n",
    "    metrics[\"Macro\"][\"AUPR\"] = np.mean(aupr_per_class)\n",
    "\n",
    "    # Weighted metrics\n",
    "    class_weights = np.bincount(y_true) / len(y_true)\n",
    "    metrics[\"Weighted\"][\"Precision\"] = np.sum(precision_per_class * class_weights)\n",
    "    metrics[\"Weighted\"][\"Recall\"] = np.sum(recall_per_class * class_weights)\n",
    "    metrics[\"Weighted\"][\"F1\"] = np.sum(f1_per_class * class_weights)\n",
    "    metrics[\"Weighted\"][\"MCC\"] = matthews_corrcoef(y_true, y_pred)\n",
    "    metrics[\"Weighted\"][\"AUPR\"] = np.average(aupr_per_class, weights=class_weights)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Training and Evaluation\n",
    "for num_estimators in nums_estimators:\n",
    "    for criterion in criteria:\n",
    "        for method in methods_max_features:\n",
    "            directory = RF_results + '/' + str(num_estimators) + ' - ' + str(criterion) + ' - ' + str(method)\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "            \n",
    "            all_metrics = {\"Micro\": [], \"Macro\": [], \"Weighted\": []}\n",
    "            \n",
    "            for fold_num, (train_indices, test_indices) in enumerate(folds, start=1):\n",
    "                fold_directory = directory + f'/fold_{fold_num}'\n",
    "                os.makedirs(fold_directory, exist_ok=True)\n",
    "                print(f'Processing Fold {fold_num}/{num_folds} ({(fold_num/num_folds)*100:.1f}%)')\n",
    "\n",
    "                \n",
    "                X_train, y_train = features[train_indices], targets[train_indices]\n",
    "                X_test, y_test = features[test_indices], targets[test_indices]\n",
    "                \n",
    "                classifier = RandomForestClassifier(\n",
    "                    n_estimators=num_estimators, criterion=criterion, max_features=method, class_weight=\"balanced\"\n",
    "                )\n",
    "                classifier.fit(X_train, y_train)\n",
    "                y_hat = classifier.predict(X_test)\n",
    "                class_probs = classifier.predict_proba(X_test)\n",
    "                \n",
    "                metrics = compute_metrics(y_test, y_hat, class_probs, num_classes)\n",
    "                for fmt in [\"Micro\", \"Macro\", \"Weighted\"]:\n",
    "                    with open(fold_directory + f'/metrics_{fmt}.txt', 'w') as fold_log_file:\n",
    "                        fold_log_file.write(f\"\\n{fmt} Metrics for Fold {fold_num}:\\n\")\n",
    "                        for metric, value in metrics[fmt].items():\n",
    "                            fold_log_file.write(f\"  {metric}: {value:.4f}\\n\")\n",
    "                    all_metrics[fmt].append(metrics[fmt])\n",
    "            \n",
    "            avg_metrics = {fmt: {metric: np.mean([fold_metrics[metric] for fold_metrics in all_metrics[fmt]])\n",
    "                                 for metric in all_metrics[fmt][0].keys()} for fmt in [\"Micro\", \"Macro\", \"Weighted\"]}\n",
    "\n",
    "\n",
    "            std_metrics = {fmt: {metric: np.std([fold_metrics[metric] for fold_metrics in all_metrics[fmt]])\n",
    "                                 for metric in all_metrics[fmt][0].keys()} for fmt in [\"Micro\", \"Macro\", \"Weighted\"]}\n",
    "\n",
    "\n",
    "\n",
    "            with open(directory + '/average_metrics.txt', 'w') as avg_file:\n",
    "                avg_file.write(\"\\nAverage Metrics Across 10 Folds:\\n\")\n",
    "                for fmt in [\"Micro\", \"Macro\", \"Weighted\"]:\n",
    "                    avg_file.write(f\"\\n{fmt} Metrics:\\n\")\n",
    "                    for metric, value in avg_metrics[fmt].items():\n",
    "                        avg_file.write(f\"  {metric}: {value:.4f} ± {std_metrics[fmt][metric]:.4f}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c326e8ed-71ce-4510-a6db-e50454ce6b32",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7bc514-8025-4cbd-aad9-c507f56c273e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1/10 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score, average_precision_score, balanced_accuracy_score, confusion_matrix\n",
    "\n",
    "# Parameters for XGBoost\n",
    "n_estimators_list = [100]\n",
    "max_depths = [3]\n",
    "learning_rates = [0.1]\n",
    "subsampling_rates = [0.5]\n",
    "colsample_bytrees = [0.5]\n",
    "\n",
    "# Read inputs for labels, indices, and total matrices\n",
    "data_dir = '../Data/'\n",
    "labels_file_path = data_dir + '31_Y_ratio1.txt'\n",
    "combinations_file_path = data_dir + '31_XIndex_ratio1.txt'\n",
    "small_molecule_matrix_file = data_dir + 'total_small_drugs.txt'\n",
    "biotech_matrix_file = data_dir + 'total_biotech_drugs.txt'\n",
    "\n",
    "\n",
    "# Load labels\n",
    "def read_label_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        return np.array([int(line.strip()) for line in lines])\n",
    "\n",
    "# Load combinations\n",
    "def read_combinations_file(file_path):\n",
    "    combinations = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            biotech_idx, small_idx = map(int, line.strip().split())\n",
    "            combinations.append((biotech_idx, small_idx))\n",
    "    return combinations\n",
    "\n",
    "labels = read_label_file(labels_file_path)\n",
    "combinations = read_combinations_file(combinations_file_path)\n",
    "\n",
    "# Load feature matrices\n",
    "small_molecule_total_matrix = np.loadtxt(small_molecule_matrix_file, dtype=float)\n",
    "biotech_total_matrix = np.loadtxt(biotech_matrix_file, dtype=float)\n",
    "\n",
    "# Generate features based on combinations\n",
    "features = np.array([\n",
    "    np.concatenate((small_molecule_total_matrix[small_idx], biotech_total_matrix[biotech_idx]))\n",
    "    for biotech_idx, small_idx in combinations\n",
    "])\n",
    "\n",
    "# Validate dimensions\n",
    "assert features.shape[0] == len(labels), \"Mismatch between number of features and labels\"\n",
    "\n",
    "results_directory = '../XGB_results'\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "XGB_results = results_directory + '/XGB'\n",
    "os.makedirs(XGB_results, exist_ok=True)\n",
    "\n",
    "# Prepare multi-class task\n",
    "targets = labels\n",
    "num_classes = 32  # Explicitly setting the number of classes\n",
    "\n",
    "num_folds = 10\n",
    "kfolder = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "folds = list(kfolder.split(features, targets))\n",
    "\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(y_true, y_pred, y_probs, num_classes):\n",
    "    metrics = {\"Micro\": {}, \"Macro\": {}, \"Weighted\": {}}\n",
    "    \n",
    "    # Micro metrics\n",
    "    metrics[\"Micro\"][\"Accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    metrics[\"Micro\"][\"Precision\"], metrics[\"Micro\"][\"Recall\"], metrics[\"Micro\"][\"F1\"], _ = \\\n",
    "        precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    metrics[\"Micro\"][\"MCC\"] = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    # Handle potential ValueError in AUC computation\n",
    "    try:\n",
    "        metrics[\"Micro\"][\"AUC\"] = roc_auc_score(y_true, y_probs, average=\"micro\", multi_class=\"ovr\")\n",
    "        metrics[\"Macro\"][\"AUC\"] = roc_auc_score(y_true, y_probs, average=\"macro\", multi_class=\"ovr\")\n",
    "        metrics[\"Weighted\"][\"AUC\"] = roc_auc_score(y_true, y_probs, average=\"weighted\", multi_class=\"ovr\")\n",
    "    except ValueError:\n",
    "        metrics[\"Micro\"][\"AUC\"] = np.nan\n",
    "        metrics[\"Macro\"][\"AUC\"] = np.nan\n",
    "        metrics[\"Weighted\"][\"AUC\"] = np.nan\n",
    "\n",
    "    metrics[\"Micro\"][\"AUPR\"] = average_precision_score(y_true, y_probs, average=\"micro\")\n",
    "\n",
    "    # Macro metrics\n",
    "    precision_per_class, recall_per_class, f1_per_class, _ = \\\n",
    "        precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)\n",
    "    aupr_per_class = [\n",
    "        average_precision_score((np.array(y_true) == i).astype(int), np.array(y_probs)[:, i])\n",
    "        for i in range(num_classes)\n",
    "    ]\n",
    "\n",
    "    metrics[\"Macro\"][\"Precision\"] = np.mean(precision_per_class)\n",
    "    metrics[\"Macro\"][\"Recall\"] = np.mean(recall_per_class)\n",
    "    metrics[\"Macro\"][\"F1\"] = np.mean(f1_per_class)\n",
    "    metrics[\"Macro\"][\"MCC\"] = matthews_corrcoef(y_true, y_pred)\n",
    "    metrics[\"Macro\"][\"AUPR\"] = np.mean(aupr_per_class)\n",
    "\n",
    "    # Weighted metrics\n",
    "    class_weights = np.bincount(y_true) / len(y_true)\n",
    "    metrics[\"Weighted\"][\"Precision\"] = np.sum(precision_per_class * class_weights)\n",
    "    metrics[\"Weighted\"][\"Recall\"] = np.sum(recall_per_class * class_weights)\n",
    "    metrics[\"Weighted\"][\"F1\"] = np.sum(f1_per_class * class_weights)\n",
    "    metrics[\"Weighted\"][\"MCC\"] = matthews_corrcoef(y_true, y_pred)\n",
    "    metrics[\"Weighted\"][\"AUPR\"] = np.average(aupr_per_class, weights=class_weights)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    for max_depth in max_depths:\n",
    "        for learning_rate in learning_rates:\n",
    "            for subsample in subsampling_rates:\n",
    "                for colsample_bytree in colsample_bytrees:\n",
    "                    config = f\"n{n_estimators}_d{max_depth}_lr{learning_rate}_sub{subsample}_col{colsample_bytree}\"\n",
    "                    directory = XGB_results + '/' + config\n",
    "                    os.makedirs(directory, exist_ok=True)\n",
    "                    \n",
    "                    all_metrics = {\"Micro\": [], \"Macro\": [], \"Weighted\": []}\n",
    "                    \n",
    "                    for fold_num, (train_indices, test_indices) in enumerate(folds, start=1):\n",
    "                        fold_directory = directory + f'/fold_{fold_num}'\n",
    "                        os.makedirs(fold_directory, exist_ok=True)\n",
    "                        print(f'Processing Fold {fold_num}/{num_folds} ({(fold_num/num_folds)*100:.1f}%)')\n",
    "                        \n",
    "                        X_train, y_train = features[train_indices], targets[train_indices]\n",
    "                        X_test, y_test = features[test_indices], targets[test_indices]\n",
    "                        \n",
    "                        classifier = xgb.XGBClassifier(\n",
    "                            n_estimators=n_estimators,\n",
    "                            max_depth=max_depth,\n",
    "                            learning_rate=learning_rate,\n",
    "                            subsample=subsample,\n",
    "                            colsample_bytree=colsample_bytree,\n",
    "                            use_label_encoder=False,\n",
    "                            eval_metric=\"mlogloss\",\n",
    "                            objective=\"multi:softprob\",  # Ensures correct probability outputs for multi-class\n",
    "                            num_class=num_classes,\n",
    "                            random_state=42\n",
    "                        )\n",
    "                        classifier.fit(X_train, y_train)\n",
    "                        y_hat = classifier.predict(X_test)\n",
    "                        class_probs = classifier.predict_proba(X_test)\n",
    "                        \n",
    "                        metrics = compute_metrics(y_test, y_hat, class_probs, num_classes)\n",
    "                        for fmt in [\"Micro\", \"Macro\", \"Weighted\"]:\n",
    "                            with open(fold_directory + f'/metrics_{fmt}.txt', 'w') as fold_log_file:\n",
    "                                fold_log_file.write(f\"\\n{fmt} Metrics for Fold {fold_num}:\\n\")\n",
    "                                for metric, value in metrics[fmt].items():\n",
    "                                    fold_log_file.write(f\"  {metric}: {value:.4f}\\n\")\n",
    "                            all_metrics[fmt].append(metrics[fmt])\n",
    "                    \n",
    "                    avg_metrics = {fmt: {metric: np.mean([fold_metrics[metric] for fold_metrics in all_metrics[fmt]])\n",
    "                                         for metric in all_metrics[fmt][0].keys()} for fmt in [\"Micro\", \"Macro\", \"Weighted\"]}\n",
    "                    std_metrics = {fmt: {metric: np.std([fold_metrics[metric] for fold_metrics in all_metrics[fmt]])\n",
    "                                         for metric in all_metrics[fmt][0].keys()} for fmt in [\"Micro\", \"Macro\", \"Weighted\"]}\n",
    "\n",
    "\n",
    "\n",
    "                    with open(directory + '/average_metrics.txt', 'w') as avg_file:\n",
    "                        avg_file.write(\"\\nAverage Metrics Across 10 Folds:\\n\")\n",
    "                        for fmt in [\"Micro\", \"Macro\", \"Weighted\"]:\n",
    "                            avg_file.write(f\"\\n{fmt} Metrics:\\n\")\n",
    "                            for metric, value in avg_metrics[fmt].items():\n",
    "                                avg_file.write(f\"  {metric}: {value:.4f} ± {std_metrics[fmt][metric]:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc3942b-26a1-4e55-b38a-58af13047881",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45df65a-dba9-4b18-9243-055b150b561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear - 10\n",
      "Processing Fold 1/10 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score, average_precision_score, balanced_accuracy_score, confusion_matrix\n",
    "\n",
    "# Parameters for SVM\n",
    "kernels = ['linear']\n",
    "Cs = [10]\n",
    "\n",
    "# Read inputs for labels, indices, and total matrices\n",
    "data_dir = '../Data/'\n",
    "labels_file_path = data_dir + '31_Y_ratio1.txt'\n",
    "combinations_file_path = data_dir + '31_XIndex_ratio1.txt'\n",
    "small_molecule_matrix_file = data_dir + 'total_small_drugs.txt'\n",
    "biotech_matrix_file = data_dir + 'total_biotech_drugs.txt'\n",
    "# Load labels\n",
    "def read_label_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        return np.array([int(line.strip()) for line in lines])\n",
    "\n",
    "# Load combinations\n",
    "def read_combinations_file(file_path):\n",
    "    combinations = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            biotech_idx, small_idx = map(int, line.strip().split())\n",
    "            combinations.append((biotech_idx, small_idx))\n",
    "    return combinations\n",
    "\n",
    "labels = read_label_file(labels_file_path)\n",
    "combinations = read_combinations_file(combinations_file_path)\n",
    "\n",
    "# Load feature matrices\n",
    "small_molecule_total_matrix = np.loadtxt(small_molecule_matrix_file, dtype=float)\n",
    "biotech_total_matrix = np.loadtxt(biotech_matrix_file, dtype=float)\n",
    "\n",
    "# Generate features based on combinations\n",
    "features = np.array([\n",
    "    np.concatenate((small_molecule_total_matrix[small_idx], biotech_total_matrix[biotech_idx]))\n",
    "    for biotech_idx, small_idx in combinations\n",
    "])\n",
    "\n",
    "# Validate dimensions\n",
    "assert features.shape[0] == len(labels), \"Mismatch between number of features and labels\"\n",
    "\n",
    "results_directory = '../SVM_results'\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "SVM_results = results_directory + '/SVM'\n",
    "os.makedirs(SVM_results, exist_ok=True)\n",
    "\n",
    "# Prepare multi-class task\n",
    "targets = labels\n",
    "num_classes = 32  # Explicitly setting the number of classes\n",
    "\n",
    "num_folds = 10\n",
    "kfolder = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "folds = list(kfolder.split(features, targets))\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(y_true, y_pred, y_probs, num_classes):\n",
    "    metrics = {\"Micro\": {}, \"Macro\": {}, \"Weighted\": {}}\n",
    "    \n",
    "    # Micro metrics\n",
    "    metrics[\"Micro\"][\"Accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    metrics[\"Micro\"][\"Precision\"], metrics[\"Micro\"][\"Recall\"], metrics[\"Micro\"][\"F1\"], _ = \\\n",
    "        precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    metrics[\"Micro\"][\"MCC\"] = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    # Handle potential ValueError in AUC computation\n",
    "    try:\n",
    "        metrics[\"Micro\"][\"AUC\"] = roc_auc_score(y_true, y_probs, average=\"micro\", multi_class=\"ovr\")\n",
    "        metrics[\"Macro\"][\"AUC\"] = roc_auc_score(y_true, y_probs, average=\"macro\", multi_class=\"ovr\")\n",
    "        metrics[\"Weighted\"][\"AUC\"] = roc_auc_score(y_true, y_probs, average=\"weighted\", multi_class=\"ovr\")\n",
    "    except ValueError:\n",
    "        metrics[\"Micro\"][\"AUC\"] = np.nan\n",
    "        metrics[\"Macro\"][\"AUC\"] = np.nan\n",
    "        metrics[\"Weighted\"][\"AUC\"] = np.nan\n",
    "\n",
    "    metrics[\"Micro\"][\"AUPR\"] = average_precision_score(y_true, y_probs, average=\"micro\")\n",
    "\n",
    "    # Macro metrics\n",
    "    precision_per_class, recall_per_class, f1_per_class, _ = \\\n",
    "        precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)\n",
    "    aupr_per_class = [\n",
    "        average_precision_score((np.array(y_true) == i).astype(int), np.array(y_probs)[:, i])\n",
    "        for i in range(num_classes)\n",
    "    ]\n",
    "\n",
    "    metrics[\"Macro\"][\"Precision\"] = np.mean(precision_per_class)\n",
    "    metrics[\"Macro\"][\"Recall\"] = np.mean(recall_per_class)\n",
    "    metrics[\"Macro\"][\"F1\"] = np.mean(f1_per_class)\n",
    "    metrics[\"Macro\"][\"MCC\"] = matthews_corrcoef(y_true, y_pred)\n",
    "    metrics[\"Macro\"][\"AUPR\"] = np.mean(aupr_per_class)\n",
    "\n",
    "    # Weighted metrics\n",
    "    class_weights = np.bincount(y_true) / len(y_true)\n",
    "    metrics[\"Weighted\"][\"Precision\"] = np.sum(precision_per_class * class_weights)\n",
    "    metrics[\"Weighted\"][\"Recall\"] = np.sum(recall_per_class * class_weights)\n",
    "    metrics[\"Weighted\"][\"F1\"] = np.sum(f1_per_class * class_weights)\n",
    "    metrics[\"Weighted\"][\"MCC\"] = matthews_corrcoef(y_true, y_pred)\n",
    "    metrics[\"Weighted\"][\"AUPR\"] = np.average(aupr_per_class, weights=class_weights)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "for kernel in kernels:\n",
    "    for C in Cs:\n",
    "        directory = SVM_results + '/' + str(kernel) + ' - ' + str(C)\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        all_metrics = {\"Micro\": [], \"Macro\": [], \"Weighted\": []}\n",
    "        \n",
    "        for fold_num, (train_indices, test_indices) in enumerate(folds, start=1):\n",
    "            fold_directory = directory + f'/fold_{fold_num}'\n",
    "            os.makedirs(fold_directory, exist_ok=True)\n",
    "            print(f'Processing Fold {fold_num}/{num_folds} ({(fold_num/num_folds)*100:.1f}%)')\n",
    "\n",
    "            X_train, y_train = features[train_indices], targets[train_indices]\n",
    "            X_test, y_test = features[test_indices], targets[test_indices]\n",
    "            \n",
    "            classifier = SVC(\n",
    "                kernel=kernel,\n",
    "                C=C,\n",
    "                probability=True,\n",
    "                decision_function_shape=\"ovr\",\n",
    "                class_weight=\"balanced\"\n",
    "            )\n",
    "            classifier.fit(X_train, y_train)\n",
    "            y_hat = classifier.predict(X_test)\n",
    "            class_probs = classifier.predict_proba(X_test)\n",
    "            \n",
    "            metrics = compute_metrics(y_test, y_hat, class_probs, num_classes)\n",
    "            for fmt in [\"Micro\", \"Macro\", \"Weighted\"]:\n",
    "                with open(fold_directory + f'/metrics_{fmt}.txt', 'w') as fold_log_file:\n",
    "                    fold_log_file.write(f\"\\n{fmt} Metrics for Fold {fold_num}:\\n\")\n",
    "                    for metric, value in metrics[fmt].items():\n",
    "                        fold_log_file.write(f\"  {metric}: {value:.4f}\\n\")\n",
    "                all_metrics[fmt].append(metrics[fmt])\n",
    "        \n",
    "        avg_metrics = {fmt: {metric: np.mean([fold_metrics[metric] for fold_metrics in all_metrics[fmt]])\n",
    "                             for metric in all_metrics[fmt][0].keys()} for fmt in [\"Micro\", \"Macro\", \"Weighted\"]}\n",
    "\n",
    "\n",
    "        std_metrics = {fmt: {metric: np.std([fold_metrics[metric] for fold_metrics in all_metrics[fmt]])\n",
    "                             for metric in all_metrics[fmt][0].keys()} for fmt in [\"Micro\", \"Macro\", \"Weighted\"]}\n",
    "\n",
    "\n",
    "\n",
    "        with open(directory + '/average_metrics.txt', 'w') as avg_file:\n",
    "            avg_file.write(\"\\nAverage Metrics Across 10 Folds:\\n\")\n",
    "            for fmt in [\"Micro\", \"Macro\", \"Weighted\"]:\n",
    "                avg_file.write(f\"\\n{fmt} Metrics:\\n\")\n",
    "                for metric, value in avg_metrics[fmt].items():\n",
    "                    avg_file.write(f\"  {metric}: {value:.4f} ± {std_metrics[fmt][metric]:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32715c39-475f-48da-b19d-9d869607749f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679b6dd-3596-4d3e-acc6-4a734e301b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6518bcc-ba26-45e1-83c8-3e483ed49d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85dade-1dc9-4078-9b2f-06d2dd1d0fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cfa13-225f-415b-bdcf-4b6c04161588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1039f5-0455-4692-8697-cd49caee001d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaef0d9-9103-4aeb-8734-1c02ad15eecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
