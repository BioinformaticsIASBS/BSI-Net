{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca31ad9-2a07-4ff9-a13d-ba1078cc137e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import dgl\n",
    "import time\n",
    "import torch\n",
    "import optuna\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from dgl.nn import GATConv\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, hamming_loss\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d9b62e-b246-4b5f-95ee-1032fd316fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose is process and organize batched data, ensuring that it can be efficiently used in the model\n",
    "def custom_collate_fn(batch):\n",
    "    graphs, d1_sim, d2_embeddings, d2_sim, labels, indices = zip(*batch)\n",
    "\n",
    "    # Batch graphs\n",
    "    batched_graphs = dgl.batch(graphs)\n",
    "\n",
    "    # Convert tensors\n",
    "    d1_sim = torch.stack(d1_sim)  # Small molecule similarity\n",
    "    d2_embeddings = torch.stack(d2_embeddings)  # Biotech embeddings\n",
    "    d2_sim = torch.stack(d2_sim)  # Biotech similarity\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    # Convert indices to tensor for the batch\n",
    "    indices = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    return batched_graphs, d1_sim, d2_embeddings, d2_sim, labels, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f741272f-50ca-4433-98d0-e56ab02c1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scientific_notation(value):\n",
    "    \"\"\"\n",
    "    Converts a value from scientific notation (or other numeric formats) to an integer.\n",
    "\n",
    "    Args:\n",
    "        value (str): Input value as a string.\n",
    "\n",
    "    Returns:\n",
    "        int or None: The integer representation of the value, or None if conversion fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return int(float(value))\n",
    "    except ValueError:\n",
    "        # Log a warning if the value cannot be converted\n",
    "        warnings.warn(f\"Could not convert value: {value}. Returning None.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2eaf608-d4fa-4d07-9deb-426be7a71b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 87752\n",
      "Number of small molecule graphs: 2148\n",
      "Small molecule total matrix shape: torch.Size([2148, 2148])\n",
      "Biotech embeddings shape: torch.Size([196, 1024])\n",
      "Biotech total matrix shape: torch.Size([196, 196])\n",
      "Number of unique labels: 32\n"
     ]
    }
   ],
   "source": [
    "def read_label_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file containing labels and converts them into integers.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [parse_scientific_notation(line.strip()) for line in lines]\n",
    "    return lines\n",
    "\n",
    "def read_combinations_index_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file containing index pairs (biotech_idx, small_molecule_idx).\n",
    "    \"\"\"\n",
    "    combinations = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            biotech_idx, small_molecule_idx = map(int, line.strip().split())\n",
    "            combinations.append((biotech_idx, small_molecule_idx))\n",
    "    return combinations\n",
    "\n",
    "# Load data files\n",
    "data_path = \"../Data/chemical_bi-directed_dgl_graphs_and_labels.pt\"\n",
    "label_file_path = \"../Data/31_Y_ratio1.txt\"\n",
    "combinations_file_path = \"../Data/31_XIndex_ratio1.txt\"\n",
    "\n",
    "try:\n",
    "    data = torch.load(data_path)\n",
    "    small_molecule_graphs, small_molecule_labels = data\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "# Process small molecule features\n",
    "small_molecule_features = []\n",
    "for g in small_molecule_graphs:\n",
    "    if 'atomic' in g.ndata:  # Node features already exist\n",
    "        feature = g.ndata['atomic']\n",
    "    else:\n",
    "        warnings.warn(f\"Graph {g} is missing 'atomic' features. Assigning default ones.\")\n",
    "        feature = torch.ones(g.num_nodes(), 1)  # Default features\n",
    "    small_molecule_features.append(feature)\n",
    "\n",
    "# Load matrices and labels\n",
    "small_molecule_total_matrix = torch.tensor(\n",
    "    np.loadtxt('../Data/total_small_drugs.txt'), dtype=torch.float\n",
    ")\n",
    "biotech_embeddings = torch.tensor(\n",
    "    np.loadtxt('../Data/mean_sequence_embeddings_Protbert2.txt'), dtype=torch.float\n",
    ")\n",
    "biotech_total_matrix = torch.tensor(\n",
    "    np.loadtxt('../Data/total_biotech_drugs.txt'), dtype=torch.float\n",
    ")\n",
    "labels_list = read_label_file(label_file_path)\n",
    "combinations = read_combinations_index_file(combinations_file_path)\n",
    "\n",
    "# Create the InteractionDataset class\n",
    "class InteractionDataset(Dataset):\n",
    "    def __init__(self, small_molecule_graphs, small_molecule_total_matrix,\n",
    "                 biotech_embeddings, biotech_total_matrix, labels, combinations):\n",
    "        \"\"\"\n",
    "        Dataset for drug-drug interactions with precomputed indices.\n",
    "        \"\"\"\n",
    "        self.small_molecule_graphs = small_molecule_graphs\n",
    "        self.small_molecule_total_matrix = small_molecule_total_matrix\n",
    "        self.biotech_embeddings = biotech_embeddings\n",
    "        self.biotech_total_matrix = biotech_total_matrix\n",
    "        self.labels = labels\n",
    "        self.combinations = combinations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.combinations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        biotech_idx, small_idx = self.combinations[idx]\n",
    "        d1_graph = self.small_molecule_graphs[small_idx]  # DGL graph\n",
    "        d1_sim = self.small_molecule_total_matrix[small_idx]  # Similarity matrix row\n",
    "        d2_embedding = self.biotech_embeddings[biotech_idx]  # ProtBERT embeddings\n",
    "        d2_sim = self.biotech_total_matrix[biotech_idx]  # Similarity matrix row\n",
    "        label = self.labels[idx]  # Label for the interaction\n",
    "\n",
    "        # Check dimensions\n",
    "        if d1_sim.shape != (2148,):\n",
    "            warnings.warn(f\"d1_sim shape mismatch: {d1_sim.shape}\")\n",
    "        if d2_embedding.shape != (1024,):\n",
    "            warnings.warn(f\"d2_embedding shape mismatch: {d2_embedding.shape}\")\n",
    "        if d2_sim.shape != (196,):\n",
    "            warnings.warn(f\"d2_sim shape mismatch: {d2_sim.shape}\")\n",
    "\n",
    "        return d1_graph, d1_sim, d2_embedding, d2_sim, label, (biotech_idx, small_idx)\n",
    "\n",
    "# Create the dataset\n",
    "labels = torch.tensor(labels_list, dtype=torch.long)  # Convert labels list to tensor\n",
    "dataset = InteractionDataset(\n",
    "    small_molecule_graphs=small_molecule_graphs,         # List of small molecule graphs\n",
    "    small_molecule_total_matrix=small_molecule_total_matrix,  # Total similarity matrix for small molecules\n",
    "    biotech_embeddings=biotech_embeddings,              # Biotech ProtBERT embeddings\n",
    "    biotech_total_matrix=biotech_total_matrix,          # Total similarity matrix for biotech drugs\n",
    "    labels=labels,                                # Labels tensor\n",
    "    combinations=combinations                           # Precomputed (biotech_idx, small_idx) pairs\n",
    ")\n",
    "\n",
    "# Debugging outputs\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of small molecule graphs: {len(small_molecule_graphs)}\")\n",
    "print(f\"Small molecule total matrix shape: {small_molecule_total_matrix.shape}\")\n",
    "print(f\"Biotech embeddings shape: {biotech_embeddings.shape}\")\n",
    "print(f\"Biotech total matrix shape: {biotech_total_matrix.shape}\")\n",
    "print(f\"Number of unique labels: {len(torch.unique(labels))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c6d7f5-c23c-4bb9-91cb-6492ac113e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sampler(dataset, indices, labels):\n",
    "    \"\"\"\n",
    "    Create a WeightedRandomSampler to handle class imbalance in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The full dataset.\n",
    "        indices (list): Indices for the subset of the dataset to sample from.\n",
    "        labels (Tensor): Full label tensor for the dataset.\n",
    "\n",
    "    Returns:\n",
    "        WeightedRandomSampler: Sampler for DataLoader.\n",
    "    \"\"\"\n",
    "    subset_labels = [labels[i].item() for i in indices]\n",
    "    class_counts = Counter(subset_labels)\n",
    "    sample_weights = [1.0 / class_counts[label] for label in subset_labels]\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(indices), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "def get_class_distribution(dataset, indices):\n",
    "    \"\"\"\n",
    "    Get the class distribution in a dataset subset.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The full dataset.\n",
    "        indices (list): Indices for the subset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of class distributions {class_label: count}.\n",
    "    \"\"\"\n",
    "    subset_labels = [dataset[i][4].item() for i in indices]\n",
    "    unique, counts = np.unique(subset_labels, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "# Define dataset indices\n",
    "indices = list(range(len(dataset)))  # List of indices for all samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ba3ea2-dacc-43ac-8daf-b904219eb9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full labels tensor size: torch.Size([87752]) (Expected: [dataset size])\n"
     ]
    }
   ],
   "source": [
    "# Generate full labels for the entire dataset\n",
    "full_labels = [dataset[i][4].item() for i in range(len(dataset))]\n",
    "full_labels = torch.tensor(full_labels, dtype=torch.long)  # Ensure correct tensor type\n",
    "print(f\"Full labels tensor size: {full_labels.shape} (Expected: [dataset size])\")\n",
    "\n",
    "# Generate indices for the entire dataset\n",
    "indices = list(range(len(dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b0c043-ebe3-4baa-a46d-e1426d748b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=32\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, out_dim, num_heads, dropout=0.2):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()  # List of LayerNorm layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        prev_dim = in_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.gat_layers.append(\n",
    "                dgl.nn.GATConv(\n",
    "                    in_feats=prev_dim,\n",
    "                    out_feats=hidden_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    allow_zero_in_degree=True,\n",
    "                )\n",
    "            )\n",
    "            self.batch_norms.append(nn.LayerNorm(hidden_dim * num_heads))  # Use LayerNorm\n",
    "            prev_dim = hidden_dim * num_heads\n",
    "\n",
    "        # Final fully connected layer\n",
    "        self.fc = nn.Linear(prev_dim, out_dim)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        for gat_layer, norm in zip(self.gat_layers, self.batch_norms):\n",
    "            h = self.dropout(h)\n",
    "            h = torch.relu(gat_layer(g, h).flatten(1))\n",
    "            h = norm(h)  # Apply LayerNorm\n",
    "        return self.fc(h)\n",
    "\n",
    "\n",
    "class MultiClassMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.2):\n",
    "        super(MultiClassMLP, self).__init__()\n",
    "        layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.LayerNorm(hidden_dim))  # Apply LayerNorm first\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(input_dim, num_classes))  # Final layer\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, gat_input_dim, gat_hidden_dims, gat_output_dim,\n",
    "                 mlp_hidden_dims, num_classes, sim_dim_small, sim_dim_biotech, dropout=0.2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.gat = GATModel(\n",
    "            in_dim=gat_input_dim,\n",
    "            hidden_dims=gat_hidden_dims,\n",
    "            out_dim=gat_output_dim,\n",
    "            num_heads=4,  # Number of attention heads\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.mlp_input_dim = gat_output_dim + sim_dim_small + 1024 + sim_dim_biotech\n",
    "        self.mlp = MultiClassMLP(self.mlp_input_dim, mlp_hidden_dims, num_classes, dropout)\n",
    "\n",
    "    def forward(self, graphs, atomic_features, batch_drug1_sim, batch_drug2_tensor, batch_drug2_sim):\n",
    "        # Process small molecule graphs through GAT\n",
    "        node_embeddings = self.gat(graphs, atomic_features)\n",
    "        graphs.ndata['h'] = node_embeddings\n",
    "        graph_embeddings = dgl.mean_nodes(graphs, 'h')  # Graph-level embeddings for the batch\n",
    "\n",
    "        # Concatenate inputs for MLP\n",
    "        drug1_input = torch.cat([batch_drug2_tensor, batch_drug1_sim], dim=1)\n",
    "        drug2_input = torch.cat([graph_embeddings, batch_drug2_sim], dim=1)\n",
    "        combined_input = torch.cat([drug1_input, drug2_input], dim=1)\n",
    "\n",
    "        return self.mlp(combined_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed112db-a0ee-4e2d-b071-c9e16194c9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Starting Config: GAT_128_256_MLP_1024_512_LR_1.0e-03_BS_32_DO_0.2\n",
      "\n",
      "[INFO] [GAT_128_256_MLP_1024_512_LR_1.0e-03_BS_32_DO_0.2] Starting Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GAT_128_256_MLP_1024_512_LR_1.0e-03_BS_32_DO_0.2] Fold 1 | Epoch 0 Training:  14%|▏| 341/2468 [01:07<07:02,  5.03batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 277\u001b[0m\n\u001b[0;32m    268\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgat_hidden_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m: [[\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m]],\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlp_hidden_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m: [[\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m512\u001b[39m]], \u001b[38;5;66;03m#[[256], [256, 512], [256, 512, 1024]]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m32\u001b[39m],\n\u001b[0;32m    274\u001b[0m }\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# Run grid search\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m \u001b[43mrun_all_combinations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 265\u001b[0m, in \u001b[0;36mrun_all_combinations\u001b[1;34m(hyperparameters, dataset, full_labels, device, save_dir)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[INFO] Starting Config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    254\u001b[0m model \u001b[38;5;241m=\u001b[39m CombinedModel(\n\u001b[0;32m    255\u001b[0m     gat_input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    256\u001b[0m     gat_hidden_dims\u001b[38;5;241m=\u001b[39mgat_hidden_dims,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m     dropout\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[0;32m    263\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 265\u001b[0m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 209\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(model, dataset, full_labels, num_splits, batch_size, device, config, config_id, save_dir)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m--> 209\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[0;32m    212\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device, fold \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, config_id, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 62\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, optimizer, criterion, device, epoch, config_id, fold)\u001b[0m\n\u001b[0;32m     60\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(graphs, graphs\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124matomic\u001b[39m\u001b[38;5;124m'\u001b[39m], d1_sim, d2_embeddings, d2_sim)\n\u001b[0;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 62\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     65\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\users\\sibco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\sibco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\sibco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\function.py:276\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[0;32m    279\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    280\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Helper function to ensure directory exists\n",
    "def ensure_dir_exists(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "\n",
    "# Save results for each fold and averages for all formats\n",
    "def save_results_to_files(config_id, fold_metrics, avg_metrics, output_dir):\n",
    "    ensure_dir_exists(output_dir)\n",
    "    formats = [\"Micro\", \"Macro\", \"Weighted\"]\n",
    "\n",
    "    for metric_format in formats:\n",
    "        file_path = os.path.join(output_dir, f\"{config_id}_{metric_format}_results.txt\")\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(f\"{metric_format} Metrics for Config: {config_id}\\n\")\n",
    "            \n",
    "            # Save fold results\n",
    "            if metric_format in fold_metrics:\n",
    "                for fold, metrics in enumerate(fold_metrics[metric_format], start=1):\n",
    "                    file.write(f\"\\nFold {fold}:\\n\")\n",
    "                    for metric, value in metrics.items():\n",
    "                        if metric.endswith(\"STD\"):\n",
    "                            file.write(f\"  {metric}: ±{value:.4f}\\n\")\n",
    "                        else:\n",
    "                            file.write(f\"  {metric}: {value:.4f}\\n\")\n",
    "            \n",
    "            # Save average metrics\n",
    "            if avg_metrics:\n",
    "                file.write(\"\\nAverage Metrics Across Folds:\\n\")\n",
    "                for metric, value in avg_metrics[metric_format].items():\n",
    "                    if metric.endswith(\"STD\"):\n",
    "                        file.write(f\"  {metric}: ±{value:.4f}\\n\")\n",
    "                    else:\n",
    "                        file.write(f\"  {metric}: {value:.4f}\\n\")\n",
    "\n",
    "    #print(f\"[INFO] Results saved for all formats in {output_dir}\")\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train(model, loader, optimizer, criterion, device, epoch, config_id, fold):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_progress = tqdm(loader, desc=f\"[{config_id}] Fold {fold} | Epoch {epoch} Training\", unit=\"batch\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_progress):\n",
    "        graphs, d1_sim, d2_embeddings, d2_sim, labels = (\n",
    "            batch[0].to(device), batch[1].to(device), batch[2].to(device),\n",
    "            batch[3].to(device), batch[4].to(device)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(graphs, graphs.ndata['atomic'], d1_sim, d2_embeddings, d2_sim)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        train_progress.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    #print(f\"[INFO] [{config_id}] Fold {fold} | Epoch {epoch} Avg Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, loader, device, fold, config_id, num_classes):\n",
    "    preds, true_labels, prob_outputs, _, _ = evaluate_and_return_preds(model, loader, device)\n",
    "\n",
    "    metrics = {\"Micro\": {}, \"Macro\": {}, \"Weighted\": {}}\n",
    "\n",
    "    # Micro metrics\n",
    "    metrics[\"Micro\"][\"Accuracy\"] = accuracy_score(true_labels, preds)\n",
    "    metrics[\"Micro\"][\"Precision\"], metrics[\"Micro\"][\"Recall\"], metrics[\"Micro\"][\"F1\"], _ = \\\n",
    "        precision_recall_fscore_support(true_labels, preds, average='micro')\n",
    "    metrics[\"Micro\"][\"MCC\"] = matthews_corrcoef(true_labels, preds)\n",
    "    metrics[\"Micro\"][\"AUC\"] = roc_auc_score(true_labels, prob_outputs, average='micro', multi_class='ovr')\n",
    "    metrics[\"Micro\"][\"AUPR\"] = np.mean([\n",
    "        average_precision_score((np.array(true_labels) == i).astype(int), np.array(prob_outputs)[:, i])\n",
    "        for i in range(num_classes)\n",
    "    ])\n",
    "\n",
    "    # Macro metrics\n",
    "    precision_per_class, recall_per_class, f1_per_class, _ = \\\n",
    "        precision_recall_fscore_support(true_labels, preds, average=None, zero_division=0)\n",
    "    auc_per_class = [\n",
    "        roc_auc_score((np.array(true_labels) == i).astype(int), np.array(prob_outputs)[:, i])\n",
    "        for i in range(num_classes)\n",
    "    ]\n",
    "    aupr_per_class = [\n",
    "        average_precision_score((np.array(true_labels) == i).astype(int), np.array(prob_outputs)[:, i])\n",
    "        for i in range(num_classes)\n",
    "    ]\n",
    "    metrics[\"Macro\"][\"Precision\"] = np.mean(precision_per_class)\n",
    "    metrics[\"Macro\"][\"Recall\"] = np.mean(recall_per_class)\n",
    "    metrics[\"Macro\"][\"F1\"] = np.mean(f1_per_class)\n",
    "    metrics[\"Macro\"][\"Precision_STD\"] = np.std(precision_per_class)\n",
    "    metrics[\"Macro\"][\"Recall_STD\"] = np.std(recall_per_class)\n",
    "    metrics[\"Macro\"][\"F1_STD\"] = np.std(f1_per_class)\n",
    "    metrics[\"Macro\"][\"AUC\"] = np.mean(auc_per_class)\n",
    "    metrics[\"Macro\"][\"AUPR\"] = np.mean(aupr_per_class)\n",
    "    metrics[\"Macro\"][\"AUC_STD\"] = np.std(auc_per_class)\n",
    "    metrics[\"Macro\"][\"AUPR_STD\"] = np.std(aupr_per_class)\n",
    "\n",
    "    # Weighted metrics\n",
    "    class_weights = np.bincount(true_labels) / len(true_labels)\n",
    "    metrics[\"Weighted\"][\"Precision\"] = np.sum(precision_per_class * class_weights)\n",
    "    metrics[\"Weighted\"][\"Recall\"] = np.sum(recall_per_class * class_weights)\n",
    "    metrics[\"Weighted\"][\"F1\"] = np.sum(f1_per_class * class_weights)\n",
    "    metrics[\"Weighted\"][\"Precision_STD\"] = np.sqrt(\n",
    "        np.sum((precision_per_class - metrics[\"Weighted\"][\"Precision\"]) ** 2 * class_weights)\n",
    "    )\n",
    "    metrics[\"Weighted\"][\"Recall_STD\"] = np.sqrt(\n",
    "        np.sum((recall_per_class - metrics[\"Weighted\"][\"Recall\"]) ** 2 * class_weights)\n",
    "    )\n",
    "    metrics[\"Weighted\"][\"F1_STD\"] = np.sqrt(\n",
    "        np.sum((f1_per_class - metrics[\"Weighted\"][\"F1\"]) ** 2 * class_weights)\n",
    "    )\n",
    "    metrics[\"Weighted\"][\"AUC\"] = np.sum(np.array(auc_per_class) * class_weights)\n",
    "    metrics[\"Weighted\"][\"AUPR\"] = np.sum(np.array(aupr_per_class) * class_weights)\n",
    "    metrics[\"Weighted\"][\"AUC_STD\"] = np.sqrt(\n",
    "        np.sum((np.array(auc_per_class) - metrics[\"Weighted\"][\"AUC\"]) ** 2 * class_weights)\n",
    "    )\n",
    "    metrics[\"Weighted\"][\"AUPR_STD\"] = np.sqrt(\n",
    "        np.sum((np.array(aupr_per_class) - metrics[\"Weighted\"][\"AUPR\"]) ** 2 * class_weights)\n",
    "    )\n",
    "\n",
    "    # Save confusion matrix\n",
    "    cm = confusion_matrix(true_labels, preds, labels=np.arange(num_classes))\n",
    "    cm_dir = \"confusion_matrices\"\n",
    "    ensure_dir_exists(cm_dir)\n",
    "    cm_file = os.path.join(cm_dir, f\"{config_id}_fold_{fold}_confusion_matrix.txt\")\n",
    "    np.savetxt(cm_file, cm, fmt='%d', delimiter='\\t')\n",
    "  #  print(f\"[INFO] Confusion Matrix for Fold {fold} saved to {cm_file}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_and_return_preds(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, true_labels, prob_outputs = [], [], []\n",
    "    biotech_indices, small_molecule_indices = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize progress bar\n",
    "        eval_progress = tqdm(loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "\n",
    "        for batch in eval_progress:\n",
    "            graphs, d1_sim, d2_embeddings, d2_sim, labels, batch_indices = (\n",
    "                batch[0].to(device), batch[1].to(device), batch[2].to(device),\n",
    "                batch[3].to(device), batch[4].to(device), batch[5]\n",
    "            )\n",
    "            outputs = model(graphs, graphs.ndata['atomic'], d1_sim, d2_embeddings, d2_sim)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            preds.extend(predictions.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            prob_outputs.extend(probabilities.cpu().numpy())\n",
    "            \n",
    "            # Unpack batch indices into biotech and small molecule indices\n",
    "            for biotech_idx, small_molecule_idx in batch_indices:\n",
    "                biotech_indices.append(biotech_idx)\n",
    "                small_molecule_indices.append(small_molecule_idx)\n",
    "\n",
    "    return preds, true_labels, prob_outputs, biotech_indices, small_molecule_indices\n",
    "\n",
    "\n",
    "\n",
    "# Save final predictions across all folds\n",
    "def save_final_predictions(config_id, final_preds, final_true_labels, final_biotech_indices, final_small_molecule_indices, save_dir):\n",
    "    ensure_dir_exists(save_dir)\n",
    "    final_predictions_file = os.path.join(save_dir, f\"{config_id}_final_predictions.txt\")\n",
    "    with open(final_predictions_file, \"w\") as file:\n",
    "        file.write(\"Biotech_Index\\tSmall_Molecule_Index\\tTrue_Label\\tPredicted_Label\\n\")\n",
    "        for biotech_idx, small_idx, true, pred in zip(final_biotech_indices, final_small_molecule_indices, final_true_labels, final_preds):\n",
    "            file.write(f\"{biotech_idx}\\t{small_idx}\\t{true}\\t{pred}\\n\")\n",
    "\n",
    "    print(f\"[INFO] Final predictions saved to {final_predictions_file}\")\n",
    "\n",
    "\n",
    "# Cross-validation function\n",
    "def cross_validate(model, dataset, full_labels, num_splits, batch_size, device, config, config_id, save_dir):\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "    all_metrics = {\"Micro\": [], \"Macro\": [], \"Weighted\": []}\n",
    "\n",
    "    # To aggregate predictions and indices across folds\n",
    "    final_preds, final_true_labels = [], []\n",
    "    final_biotech_indices, final_small_molecule_indices = [], []\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(skf.split(range(len(dataset)), full_labels.numpy())):\n",
    "        print(f\"\\n[INFO] [{config_id}] Starting Fold {fold + 1}/{num_splits}\")\n",
    "        train_set = Subset(dataset, train_indices)\n",
    "        val_set = Subset(dataset, val_indices)\n",
    "\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "        val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # Training\n",
    "        for epoch in range(10):  # Adjust epochs as needed\n",
    "            train(model, train_loader, optimizer, criterion, device, epoch, config_id, fold + 1)\n",
    "\n",
    "        # Evaluation\n",
    "        preds, true_labels, prob_outputs, biotech_indices, small_molecule_indices = evaluate_and_return_preds(\n",
    "            model, val_loader, device\n",
    "        )\n",
    "\n",
    "        # Aggregate predictions and indices\n",
    "        final_preds.extend(preds)\n",
    "        final_true_labels.extend(true_labels)\n",
    "        final_biotech_indices.extend(biotech_indices)\n",
    "        final_small_molecule_indices.extend(small_molecule_indices)\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = evaluate(model, val_loader, device, fold + 1, config_id, num_classes=32)\n",
    "\n",
    "        # Store metrics for this fold\n",
    "        for fmt in [\"Micro\", \"Macro\", \"Weighted\"]:\n",
    "            format_metrics = {metric: metrics[fmt][metric] for metric in metrics[fmt]}\n",
    "            all_metrics[fmt].append(format_metrics)\n",
    "\n",
    "        # Save fold results immediately\n",
    "        fold_results_dir = os.path.join(save_dir, f\"fold_{fold + 1}\")\n",
    "        ensure_dir_exists(fold_results_dir)\n",
    "        save_results_to_files(config_id, {fmt: [metrics[fmt]] for fmt in metrics}, {}, fold_results_dir)\n",
    "\n",
    "    # Compute average metrics across folds\n",
    "    avg_metrics = {}\n",
    "    for fmt in [\"Micro\", \"Macro\", \"Weighted\"]:\n",
    "        avg_metrics[fmt] = {}\n",
    "        for metric in all_metrics[fmt][0].keys():\n",
    "            values = [fold_metrics[metric] for fold_metrics in all_metrics[fmt]]\n",
    "            avg_metrics[fmt][metric] = np.mean(values)\n",
    "            if not metric.endswith(\"STD\"):\n",
    "                avg_metrics[fmt][f\"{metric}_STD\"] = np.std(values)\n",
    "\n",
    "    # Save overall average metrics\n",
    "    avg_results_dir = os.path.join(save_dir, \"averages\")\n",
    "    ensure_dir_exists(avg_results_dir)\n",
    "    save_results_to_files(config_id, all_metrics, avg_metrics, avg_results_dir)\n",
    "\n",
    "    # Save predictions after all folds\n",
    "    save_final_predictions(\n",
    "        config_id=config_id,\n",
    "        final_preds=final_preds,\n",
    "        final_true_labels=final_true_labels,\n",
    "        final_biotech_indices=final_biotech_indices,\n",
    "        final_small_molecule_indices=final_small_molecule_indices,\n",
    "        save_dir=save_dir,\n",
    "    )\n",
    "\n",
    "    return avg_metrics\n",
    "\n",
    "    # Save overall average metrics\n",
    "    avg_results_dir = os.path.join(save_dir, \"averages\")\n",
    "    ensure_dir_exists(avg_results_dir)\n",
    "    save_results_to_files(config_id, all_metrics, avg_metrics, avg_results_dir)\n",
    "\n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter grid search\n",
    "def run_all_combinations(hyperparameters, dataset, full_labels, device, save_dir):\n",
    "    for gat_hidden_dims in hyperparameters[\"gat_hidden_dims\"]:\n",
    "        for mlp_hidden_dims in hyperparameters[\"mlp_hidden_dims\"]:\n",
    "            for dropout in hyperparameters[\"dropout\"]:\n",
    "                for learning_rate in hyperparameters[\"learning_rate\"]:\n",
    "                    for batch_size in hyperparameters[\"batch_size\"]:\n",
    "                        config = {\n",
    "                            \"gat_hidden_dims\": gat_hidden_dims,\n",
    "                            \"mlp_hidden_dims\": mlp_hidden_dims,\n",
    "                            \"dropout\": dropout,\n",
    "                            \"learning_rate\": learning_rate,\n",
    "                            \"batch_size\": batch_size,\n",
    "                        }\n",
    "                        config_id = f\"GAT_{'_'.join(map(str, gat_hidden_dims))}_MLP_{'_'.join(map(str, mlp_hidden_dims))}_LR_{learning_rate:.1e}_BS_{batch_size}_DO_{dropout}\"\n",
    "                        print(f\"\\n[INFO] Starting Config: {config_id}\")\n",
    "\n",
    "                        model = CombinedModel(\n",
    "                            gat_input_dim=1,\n",
    "                            gat_hidden_dims=gat_hidden_dims,\n",
    "                            gat_output_dim=512,\n",
    "                            mlp_hidden_dims=mlp_hidden_dims,\n",
    "                            num_classes=num_classes,\n",
    "                            sim_dim_small=2148,\n",
    "                            sim_dim_biotech=196,\n",
    "                            dropout=dropout,\n",
    "                        ).to(device)\n",
    "\n",
    "                        cross_validate(model, dataset, full_labels, num_splits=10, batch_size=batch_size, device=device, config=config, config_id=config_id, save_dir=save_dir)\n",
    "\n",
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    \"gat_hidden_dims\": [[128, 256]],\n",
    "    \"mlp_hidden_dims\": [[1024, 512]], #[[256], [256, 512], [256, 512, 1024]]\n",
    "    \"dropout\": [0.2],\n",
    "    \"learning_rate\": [1e-3],\n",
    "    \"batch_size\": [32],\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "run_all_combinations(hyperparameters, dataset, full_labels, device=device, save_dir=\"results\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914cf56b-42da-4390-80be-ddf4d40d8eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df06cfe-b76f-4232-857a-0d9095988981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de8e76-4e19-43e6-aed0-57d0aed7539e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a1814-6352-4fd2-b093-17afb51064c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc33472-7ae6-4c49-8c6e-5fc5ae2f86fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3dc767-2bdf-4048-8f4b-3ff7f9ab16be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf46f5c-6049-4c7f-822d-9b3d8092a7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2c4b0-bb58-473b-a12f-cbecc4f53eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b1aa7-9f33-41e9-a76a-09a6f138e41f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
